{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that it should be possible to \"diff\" models like Anthropic does in their crosscoder paper, without training a crosscoder! \n",
    "\n",
    "We can think of a transformer $T$ as a function from an activation vector to a piece of text in semantic space. Defining addition in this semantic space is kind of tricky, but we say that if we add together two pieces of text that are \"happy\", we get a piece of text that is happier.\n",
    "\n",
    "From the initial paper on sparse autoencoders, we know that for two feature vectors $U, V$:\n",
    "\n",
    "$T(U + V) \\approx T(U) + T(V)$\n",
    "\n",
    "Take this approx with a huge grain of salt, but we can then say that this transformer is a linear map between the SAE feature space and the semantic space.\n",
    "\n",
    "If for two different transformers $T_1, T_2$ acting on activation spaces $A_1, A_2$, we can find a linear map from $A_1$ to $A_2$ that best aligns the two models, and because the transformer is linear, this alignment should be pretty good! We can then look at where this alignment fails and use that to understand differences between the two models.\n",
    "\n",
    "This linear map from $A_1$ to $A_2$ is like a change of basis between the two model activation spaces.\n",
    "\n",
    "Basically, you can find a transformation between the features of one model and the features of another model, and where this transformation fails is where the two models have the most difference.\n",
    "\n",
    "In this post / notebook, I show emperical evidence that this may be possible!\n",
    "\n",
    "## Method\n",
    "\n",
    "The method is as follows:\n",
    "\n",
    "I first learn a linear map between activation spaces $A_1$ and $A_2$ of two different models $T_1, T_2$. I do this by collecting a dataset of activation vectors from each model on the same pieces of text, and learn the linear map $R$ that best converts between the two activation spaces.\n",
    "\n",
    "I then take the features that form the basis for the feature spaces $F_1, F_2$. We can convert all the features in $F_1$ to the $F_2$ space using the linear map $R$, but I don't know correspondence between the features in $F_1$ and $F_2$. Thus, I use the linear assignment solver / hungarian algorithm to find the best permutation of the features in $F_1$ that minimizes the distance to the features in $F_2$. \n",
    "\n",
    "I take this alignment for granted, and say that it is perfect. Then, we can say that this is basically the same thing as the crosscoder vector, and just like the crosscoder we can look at the encoder norms to the permuted features in $F_1$ and the encoder norms to the features in $F_2$. Where these differ the most is the features where the models have the most difference.\n",
    "\n",
    "In order to test this, I use the self-interp method introduced in [Self-explaining SAE Features (Kharlapenko et al.)](https://www.alignmentforum.org/posts/8ev6coxChSWcxCDy8/self-explaining-sae-featuress). This is not ideal, and having the dashboards for this SAE would be best, but I don't have access to this.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "I provide a lot of code here. Feel free to skip to the section \"Conclusions\" if you want to see the results.\n",
    "\n",
    "We first generate the data for points to align.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"\"\"This code isn't meant to be run with the rest of the code. \n",
    "                It generates the data for the rest of the code, and this seems\n",
    "                bad to run again everytime you want to do the experiment.\n",
    "                Comment this out!\"\"\")\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = \"mps\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "gemma_2b_it = HookedTransformer.from_pretrained(\n",
    "    \"gemma-2b-it\",\n",
    "    torch_dtype=torch.bfloat16, \n",
    ").to(device)\n",
    "dataset = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
    "dataset_iter = iter(dataset)\n",
    "first_500_points = [next(dataset_iter)[\"text\"] for _ in range(500)]\n",
    "tokens = gemma_2b_it.to_tokens(\n",
    "    first_500_points, prepend_bos=True, padding_side=\"left\", move_to_device=True\n",
    ").detach()[:, :2048]\n",
    "\n",
    "# prepare hooks\n",
    "gemma_2b_activations = []\n",
    "gemma_2b_it_activations = []\n",
    "\n",
    "print(tokens.shape)\n",
    "\n",
    "def gemma_2b_hook(activations, hook):\n",
    "    act = activations[..., -1, :].detach().cpu()  # Keep on GPU\n",
    "    gemma_2b_activations.append(act)\n",
    "\n",
    "def gemma_2b_it_hook(activations, hook):\n",
    "    act = activations[..., -1, :].detach().cpu()  # Keep on GPU\n",
    "    gemma_2b_it_activations.append(act)\n",
    "\n",
    "# Run inference\n",
    "BATCH_SIZE = 5\n",
    "for i in tqdm(range(0, len(tokens), BATCH_SIZE)):\n",
    "    batch = tokens[i:i + BATCH_SIZE]\n",
    "    ans = gemma_2b_it.run_with_hooks(\n",
    "        batch,\n",
    "        fwd_hooks=[(\"blocks.12.hook_resid_post\", gemma_2b_it_hook)],\n",
    "    )\n",
    "    del ans\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "del gemma_2b_it\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load second model to GPU with bf16\n",
    "gemma_2b = HookedTransformer.from_pretrained(\n",
    "    \"gemma-2b\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "\n",
    "for i in tqdm(range(0, len(tokens), BATCH_SIZE)):\n",
    "    batch = tokens[i:i + BATCH_SIZE]\n",
    "    ans = gemma_2b.run_with_hooks(\n",
    "        batch,\n",
    "        fwd_hooks=[(\"blocks.12.hook_resid_post\", gemma_2b_hook)],\n",
    "    )\n",
    "    del ans\n",
    "    torch.cuda.empty_cache()\n",
    "# concatenate activations (still on GPU)\n",
    "gemma_2b_activations = torch.cat(gemma_2b_activations, dim=0)\n",
    "gemma_2b_it_activations = torch.cat(gemma_2b_it_activations, dim=0)\n",
    "\n",
    "del tokens, gemma_2b\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Move to CPU only when saving\n",
    "torch.save(gemma_2b_activations, \"similarity_of_sae_dir/gemma_2b_activations.pt\")\n",
    "torch.save(gemma_2b_it_activations, \"similarity_of_sae_dir/gemma_2b_it_activations.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load this dataset and learn the linear map between the two activation spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2048) (500, 2048)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from numpy.linalg import norm\n",
    "\n",
    "gemma_2b_activations = torch.load(\"similarity_of_sae_dir/gemma_2b_activations.pt\", weights_only=True).float().cpu().numpy()\n",
    "gemma_2b_it_activations = torch.load(\"similarity_of_sae_dir/gemma_2b_it_activations.pt\", weights_only=True).float().cpu().numpy()\n",
    "\n",
    "print(gemma_2b_activations.shape, gemma_2b_it_activations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 3.2549355296672035e-12\n",
      "Frobenius norm of error: 0.0018256653533382547\n",
      "Frobenius norm of R: 0.24972663642939782\n"
     ]
    }
   ],
   "source": [
    "# First compute A^T A with regularization term\n",
    "lambda_reg = 1.0  # L2 regularization strength\n",
    "n_features = gemma_2b_activations.shape[1]\n",
    "ATA = gemma_2b_activations.T @ gemma_2b_activations + lambda_reg * np.eye(n_features)\n",
    "\n",
    "# Then compute A^T B \n",
    "ATB = gemma_2b_activations.T @ gemma_2b_it_activations\n",
    "\n",
    "# Solve for R with regularization\n",
    "R = np.linalg.solve(ATA, ATB)\n",
    "\n",
    "# Compute the transformed activations\n",
    "transformed_activations = gemma_2b_activations @ R\n",
    "\n",
    "# Calculate error\n",
    "error = np.mean((transformed_activations - gemma_2b_it_activations) ** 2)\n",
    "print(f\"Mean squared error: {error}\")\n",
    "\n",
    "# Print the frobenius norm of the error\n",
    "frobenius_norm_error = norm(transformed_activations - gemma_2b_it_activations, \"fro\")\n",
    "print(f\"Frobenius norm of error: {frobenius_norm_error}\")\n",
    "\n",
    "# Print the frobenius norm of R to check if regularization helped\n",
    "print(f\"Frobenius norm of R: {norm(R, 'fro')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the SAEs and get the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# disable gradients\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "gemma_2b_sae, gemma_2b_cfg, gemma_2b_sparsity = SAE.from_pretrained(\n",
    "    release=\"gemma-2b-res-jb\",\n",
    "    sae_id=\"blocks.12.hook_resid_post\",\n",
    "    device=\"mps\"\n",
    ")\n",
    "\n",
    "gemma_2b_it_sae, gemma_2b_it_cfg, gemma_2b_it_sparsity = SAE.from_pretrained(\n",
    "    release=\"gemma-2b-it-res-jb\",\n",
    "    sae_id=\"blocks.12.hook_resid_post\",\n",
    "    device=\"mps\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16384, 2048) (16384, 2048)\n",
      "16384 16384\n"
     ]
    }
   ],
   "source": [
    "gemma_2b_dec = gemma_2b_sae.W_dec.detach().cpu().numpy()\n",
    "gemma_2b_it_dec = gemma_2b_it_sae.W_dec.detach().cpu().numpy()\n",
    "\n",
    "gemma_2b_enc = gemma_2b_sae.W_enc.detach().cpu().numpy()\n",
    "gemma_2b_it_enc = gemma_2b_it_sae.W_enc.detach().cpu().numpy()\n",
    "\n",
    "m_A = gemma_2b_dec.shape[0]\n",
    "m_B = gemma_2b_it_dec.shape[0]\n",
    "n = gemma_2b_dec.shape[1]\n",
    "\n",
    "print(gemma_2b_dec.shape, gemma_2b_it_dec.shape)\n",
    "print(len(gemma_2b_dec), len(gemma_2b_it_dec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this learnt lienar map to the SAE features, and then learn a permutation that best aligns the two sets of features. *Warning*: This takes a while to run, because learning the permutation sucks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16384/16384 [00:52<00:00, 311.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error after alignment: 0.0008108550100587308\n",
      "First few permutation mappings:\n",
      "Feature 0 maps to feature 8785\n",
      "Feature 1 maps to feature 8580\n",
      "Feature 2 maps to feature 2406\n",
      "Feature 3 maps to feature 5323\n",
      "Feature 4 maps to feature 1309\n",
      "Feature 5 maps to feature 9431\n",
      "Feature 6 maps to feature 5123\n",
      "Feature 7 maps to feature 6775\n",
      "Feature 8 maps to feature 1934\n",
      "Feature 9 maps to feature 1510\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Convert numpy arrays to torch tensors on MPS\n",
    "transformed_features = gemma_2b_dec @ R\n",
    "instruct_features = gemma_2b_it_dec\n",
    "transformed_features_t = torch.from_numpy(transformed_features.astype(np.float32)).to('mps')\n",
    "instruct_features_t = torch.from_numpy(instruct_features.astype(np.float32)).to('mps')\n",
    "\n",
    "# Calculate pairwise distances between all features using torch\n",
    "distances = torch.zeros((transformed_features.shape[0], instruct_features.shape[0]), device='mps')\n",
    "for i in tqdm(range(transformed_features.shape[0])):\n",
    "    # Vectorize inner loop by broadcasting\n",
    "    diff = transformed_features_t[i:i+1] - instruct_features_t\n",
    "    distances[i] = torch.norm(diff, dim=1)\n",
    "\n",
    "# Move distances back to CPU for linear_sum_assignment\n",
    "distances = distances.cpu().numpy()\n",
    "\n",
    "# Find optimal assignment that minimizes total distance\n",
    "row_ind, col_ind = linear_sum_assignment(distances)\n",
    "\n",
    "# Create permutation matrix from the assignment\n",
    "permutation = torch.zeros((len(row_ind), len(col_ind)), device='mps')\n",
    "for i, j in zip(row_ind, col_ind):\n",
    "    permutation[i,j] = 1\n",
    "\n",
    "# Apply permutation to get aligned features\n",
    "aligned_features = permutation @ transformed_features_t\n",
    "\n",
    "# Calculate error after alignment\n",
    "error_after_alignment = torch.mean((aligned_features - instruct_features_t) ** 2).item()\n",
    "print(f\"Mean squared error after alignment: {error_after_alignment}\")\n",
    "\n",
    "# Store the permutation indices for later use with encoder\n",
    "permutation_indices = col_ind\n",
    "\n",
    "print(\"First few permutation mappings:\")\n",
    "for i in range(min(10, len(row_ind))):\n",
    "    print(f\"Feature {i} maps to feature {permutation_indices[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we take the norms to this common basis, and see where the largest differences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute norm difference: 0.1373\n",
      "Max absolute norm difference: 0.7869\n",
      "Min absolute norm difference: 0.0000\n",
      "Largest positive differences (index, value):\n",
      "4988 -> 10841: 0.6487\n",
      "4424 -> 5233: 0.5897\n",
      "1722 -> 8731: 0.5539\n",
      "11453 -> 1589: 0.3922\n",
      "12141 -> 4834: 0.3907\n",
      "16381 -> 16317: 0.3683\n",
      "10706 -> 14482: 0.3635\n",
      "15868 -> 15497: 0.3632\n",
      "1365 -> 9630: 0.3552\n",
      "8682 -> 1547: 0.3484\n",
      "\n",
      "Largest negative differences (index, value):\n",
      "12816: -0.7869\n",
      "13319: -0.7215\n",
      "1551: -0.6639\n",
      "13208: -0.6596\n",
      "4503: -0.6468\n",
      "6440: -0.6290\n",
      "14235: -0.6024\n",
      "2982: -0.6017\n",
      "6278: -0.6017\n",
      "16007: -0.5942\n"
     ]
    }
   ],
   "source": [
    "# Get encoder norms in permuted space\n",
    "encoder_weights = gemma_2b_enc.T\n",
    "permuted_encoder_norms = np.linalg.norm(encoder_weights[permutation_indices], axis=1) / np.max(np.linalg.norm(encoder_weights[permutation_indices], axis=1))\n",
    "\n",
    "# Get encoder norms in instruct space\n",
    "instruct_encoder_norms = np.linalg.norm(gemma_2b_it_enc.T, axis=1) / np.max(np.linalg.norm(gemma_2b_it_enc.T, axis=1))\n",
    "\n",
    "# Calculate differences in norms\n",
    "norm_differences = permuted_encoder_norms - instruct_encoder_norms\n",
    "\n",
    "# Plot histogram of norm differences\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(norm_differences, bins=50)\n",
    "plt.title('Histogram of Norm Differences (Permuted - Instruct)')\n",
    "plt.xlabel('Norm Difference')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Mean absolute norm difference: {np.mean(np.abs(norm_differences)):.4f}\")\n",
    "print(f\"Max absolute norm difference: {np.max(np.abs(norm_differences)):.4f}\")\n",
    "print(f\"Min absolute norm difference: {np.min(np.abs(norm_differences)):.4f}\")\n",
    "\n",
    "# get the indices of the largest positive norm differences\n",
    "# and the largest negative norm differences\n",
    "largest_positive_indices = np.argsort(norm_differences)[::-1]\n",
    "largest_negative_indices = np.argsort(norm_differences)\n",
    "\n",
    "print(\"Largest positive differences (index, value):\")\n",
    "for idx in largest_positive_indices[:10]:\n",
    "    print(f\"{idx} -> {permutation_indices[idx]}: {norm_differences[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nLargest negative differences (index, value):\")\n",
    "for idx in largest_negative_indices[:10]:\n",
    "    print(f\"{idx}: {norm_differences[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have identified the largest positive and negative diffences. For reference, positive differences are features that exist in the base model, while negative differences are features that exist in the instruct model.\n",
    "\n",
    "```\n",
    "Positive: [ 4988  4424  1722 11453 12141]\n",
    "Negative: [12816 13319  1551 13208  4503]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075ae47b29c14b9bba1dec3ded09f568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0480d933f2e64c3d9eb33d587cc1eea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067e66a1db47456fa904a29a1119869e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f29919a4c54d23ad3f3e8455332048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fe7869f0e742da90885584e63af4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b698fc9b2b44e781c5d35040af1337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5df5dffdf44b85bceb9fe3766f41a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637ad62f49f24cdfb5b142edf836e7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24103c1523d541f99581adfbbe09fa53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65170fdde2046fd9bdb1c0c2a6c7a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1ce5521e4e41d980e1751d8f88e556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f04c6dc50ec4f5491e88abd01083a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b-it into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc706bb81e0493fa1bdb24609ca1305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4f81ce0dab4a93993da67b930f6409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c44581d1d1641dbbfacf69be11065a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb59249600874aa78db6a4ae56815d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee0a543a07743de916c6b1bdf32351c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e11bd9111544eab6d6c83c70558886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80b94696fe54ec6b8f845a8755e6fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cfa716badd846e19f0e89a7097825b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654558e96d704736b82bfe7d5330b304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a294232d784943b90f68e85fd7736e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONLY IN BASE MODEL:\n",
      "Steering with feature 10841: The meaning of X<bos><h1>X-ray diffraction</h1>\n",
      "\n",
      "<b>X-ray diffraction</b> is a technique used to determine the structure of crystals. It was discovered by Max\n",
      "****************************************************************************************************\n",
      "Steering with feature 5233: The meaning of X<bos><h1>How to get the current page URL in a custom module?</h1>\n",
      "\n",
      "I'm trying to create a custom module that will display the current\n",
      "****************************************************************************************************\n",
      "Steering with feature 8731: The meaning of X<bos><h1>How to get the current time in a specific timezone</h1>\n",
      "\n",
      "I'm trying to get the current time in a specific timezone. I have\n",
      "****************************************************************************************************\n",
      "Steering with feature 1589: The meaning of X want to be the same as the meaning of 1.\n",
      "\n",
      "The first is a person who wants to be a star, and the second is a\n",
      "****************************************************************************************************\n",
      "Steering with feature 4834: The meaning of X said:\n",
      "\n",
      "1. The meaning of the word \"word\" is to use words to express something, and the word \"word\" is a kind\n",
      "****************************************************************************************************\n",
      "Steering with feature 16317: The meaning of Xlindo’s name is “<b>A beautiful girl</b>” and it is an <b>Girl name</b>.\n",
      "The origin of the name <b>\n",
      "****************************************************************************************************\n",
      "Steering with feature 14482: The meaning of X Ubic is \"<b>Of the sea</b>\".\n",
      "Its origin is \"<b>Modern English variant of the Latin name Ubiquitus</b>\".\n",
      "The list\n",
      "****************************************************************************************************\n",
      "Steering with feature 15497: The meaning of X10 is “the first”.\n",
      "\n",
      "The meaning of 10 is “ten”.\n",
      "\n",
      "The number 10 has a lot of meanings.\n",
      "****************************************************************************************************\n",
      "Steering with feature 9630: The meaning of X Karang is \"<b>The first born</b>\".\n",
      "Its origin is \"<b>Modern English variant of the Irish name O'Carragh</b>\".\n",
      "\n",
      "****************************************************************************************************\n",
      "Steering with feature 1547: The meaning of X itself is unknown, but it is known that the number 13 was used as a symbol for the devil in many cultures. The number 1\n",
      "\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "ONLY IN INSTRUCT MODEL:\n",
      "Steering with feature 12816: <start_of_turn>user\n",
      "What is the meaning of X?<end_of_turn>\n",
      "The context does not provide any information about the meaning of \"what is the meaning of X?\". Therefore, I cannot answer this question from the provided context\n",
      "****************************************************************************************************\n",
      "Steering with feature 13319: <start_of_turn>user\n",
      "What is the meaning of X?<end_of_turn>\n",
      "The context does not provide any information about the meaning of \"there\".\n",
      "\n",
      "**Possible interpretations:**\n",
      "\n",
      "* The variable or object being referred to in a\n",
      "****************************************************************************************************\n",
      "Steering with feature 1551: <start_of_turn>user\n",
      "What is the meaning of X?<end_of_turn>\n",
      "The context does not specify anything about the meaning of \"what is the meaning of X?\". Therefore, I cannot answer this question from the provided context.\n",
      "****************************************************************************************************\n",
      "Steering with feature 13208: <start_of_turn>user\n",
      "What is the meaning of X?<end_of_turn>\n",
      "The context does not provide any information about the meaning of \"what is the meaning of\". Therefore, I cannot answer this question from the provided context.\n",
      "****************************************************************************************************\n",
      "Steering with feature 4503: <start_of_turn>user\n",
      "What is the meaning of X?<end_of_turn>\n",
      "The context does not specify anything about the meaning of \"what is the meaning of X?\". Therefore, I cannot answer this question from the provided context.\n",
      "****************************************************************************************************\n",
      "Steering with feature 6440: <start_of_turn>user\n",
      "What is the meaning of X?<end_of_turn>\n",
      "The context does not specify anything about the meaning of \"what is the meaning of X?\". Therefore, I cannot answer this question from the provided context.\n",
      "****************************************************************************************************\n",
      "Steering with feature 14235: <start_of_turn>user\n",
      "What is the meaning of X?<end_of_turn>\n",
      "The context does not provide any information about the meaning of \"what is the meaning of\". Therefore, I cannot answer this question from the provided context.\n",
      "****************************************************************************************************\n",
      "Steering with feature 2982: <start_of_turn>user\n",
      "What is the meaning of X?<end_of_turn>\n",
      "The context does not provide any information about the meaning of \"the\". Therefore, I cannot answer this question from the provided context.<eos><eos><eos>The\n",
      "****************************************************************************************************\n",
      "Steering with feature 6278: <start_of_turn>user\n",
      "What is the meaning of X?<end_of_turn>\n",
      "The context does not provide any information about the meaning of \"what is the meaning of\". Therefore, I cannot answer this question from the provided context.\n",
      "****************************************************************************************************\n",
      "Steering with feature 16007: <start_of_turn>user\n",
      "What is the meaning of X?<end_of_turn>\n",
      "The context does not provide any information about the meaning of \"what is the meaning of X?\". Therefore, I cannot answer this question from the provided context\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "import torch\n",
    "\n",
    "gemma_2b_model = HookedTransformer.from_pretrained(\"gemma-2b\", device=\"mps\", dtype=torch.bfloat16)\n",
    "gemma_2b_sae, cfg_dict, _ = SAE.from_pretrained(\n",
    "    release=\"gemma-2b-res-jb\", sae_id=\"blocks.12.hook_resid_post\", device=\"mps\"\n",
    ")\n",
    "\n",
    "positive_outputs = []\n",
    "negative_outputs = []\n",
    "\n",
    "steering_coeff = 100\n",
    "prompt = \"The meaning of X\"\n",
    "instruct_prompt = \"What is the meaning of X?\"\n",
    "def steer_with_feature(feature_index: int, prompt: str, model: HookedTransformer, sae: SAE) -> str:\n",
    "    steering_vector = sae.W_dec[feature_index]\n",
    "    def steering_hook(resid_post, hook):\n",
    "        if resid_post.shape[1] == 1:\n",
    "            return\n",
    "\n",
    "        resid_post[:, 4, :] += steering_coeff * steering_vector\n",
    "        return resid_post\n",
    "    \n",
    "    sampling_kwargs = dict(temperature=1.0, top_p=0.1, freq_penalty=1.0)\n",
    "    with model.hooks(fwd_hooks=[(\"blocks.12.hook_resid_post\", steering_hook)]):\n",
    "        tokenized = model.to_tokens(prompt, prepend_bos=True)\n",
    "        result = model.generate(\n",
    "            stop_at_eos=False, # avoids a bug on MPS\n",
    "            input=tokenized,\n",
    "            max_new_tokens = 30,\n",
    "            do_sample=True,\n",
    "            **sampling_kwargs\n",
    "        )\n",
    "        return f\"Steering with feature {feature_index}: {''.join(model.to_str_tokens(result[0])[1:])}\"\n",
    "\n",
    "def steer_with_feature_instruct(feature_index: int, prompt: str, model: HookedTransformer, sae: SAE) -> str:\n",
    "    steering_vector = sae.W_dec[feature_index]\n",
    "    def steering_hook(resid_post, hook):\n",
    "        if resid_post.shape[1] == 1:\n",
    "            return\n",
    "\n",
    "        resid_post[:, 9, :] += 1000 * steering_vector\n",
    "        return resid_post\n",
    "    \n",
    "    sampling_kwargs = dict(temperature=1.0, top_p=0.1, freq_penalty=1.0)\n",
    "    with model.hooks(fwd_hooks=[(\"blocks.12.hook_resid_post\", steering_hook)]):\n",
    "        tokenized = model.to_tokens(\n",
    "            model.tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"user\", \"content\": prompt}], \n",
    "                tokenize=False\n",
    "            ), prepend_bos=False\n",
    "        )\n",
    "        result = model.generate(\n",
    "            stop_at_eos=False, # avoids a bug on MPS\n",
    "            input=tokenized,\n",
    "            max_new_tokens = 30,\n",
    "            do_sample=True,\n",
    "            **sampling_kwargs\n",
    "        )\n",
    "        return f\"Steering with feature {feature_index}: {''.join(model.to_str_tokens(result[0])[1:])}\"\n",
    "    \n",
    "for i in [10841, 5233, 8731, 1589, 4834, 16317, 14482, 15497, 9630, 1547]:\n",
    "    positive_outputs.append(steer_with_feature(i, prompt, model=gemma_2b_model, sae=gemma_2b_sae))\n",
    "\n",
    "del gemma_2b_model # free up memory\n",
    "\n",
    "gemma_2b_it = HookedTransformer.from_pretrained(\"gemma-2b-it\", device=\"mps\", dtype=torch.bfloat16)\n",
    "gemma_2b_it_sae, cfg_dict, _ = SAE.from_pretrained(\n",
    "    release=\"gemma-2b-it-res-jb\", sae_id=\"blocks.12.hook_resid_post\", device=\"mps\"\n",
    ")\n",
    "\n",
    "for i in [12816, 13319, 1551, 13208, 4503, 6440, 14235, 2982, 6278, 16007]:\n",
    "    negative_outputs.append(steer_with_feature_instruct(i, instruct_prompt, model=gemma_2b_it, sae=gemma_2b_it_sae))\n",
    "\n",
    "del gemma_2b_it_sae # free up memory\n",
    "del gemma_2b_it # free up memory\n",
    "\n",
    "print(\"ONLY IN BASE MODEL:\")\n",
    "print((\"\\n\" + \"*\" * 100 + \"\\n\").join(positive_outputs))\n",
    "print(\"\\n\\n\")\n",
    "print(\"*\" * 100)\n",
    "print(\"\\n\\n\")\n",
    "print(\"ONLY IN INSTRUCT MODEL:\")\n",
    "print((\"\\n\" + \"*\" * 100 + \"\\n\").join(negative_outputs))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
